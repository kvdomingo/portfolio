---
title: "Activity 16: Support Vector Machines"
keywords:
  - image processing
  - computer vision
  - support vector machines
  - feature extraction
  - object classification
cover: "svip/186/16-SVM/svm-db.png"
courseSlug: "ap186"
created: "2019-11-21T07:14:01.0000Z"

alphabet: abcdefghijklmnopqrstuvwxyz
---

import { buildCldUrl } from "@/utils/cloudinary";

For this activity [[1](#Soriano2019), [2](#Veksler)], I used the extracted fruit color features in $a^*-b^*$ space from
a [previous activity](/svip/ap186/feature-extract). I used only the data for oranges and apples, and assigned them
labels of $-1$ \& $+1$, respectively. The objective for SVM is

<span id="eq-objective" class="mx-auto">
$
\begin{equation}
  \min \frac{1}{2} |{\vec{w}}|_2^2 \quad \textrm{subject to} \quad y_i (\vec{w}^\top \vec{x}_i + b) \geq 1 \quad \forall i
\end{equation}
$
</span>

which is a quadratic and hence, convex problem. Here, $\vec{w}$ is the reference vector perpendicular to the decision
line, $\vec{x}$ is the feature vector, $y$ is the output classification, and $b$ is the bias. Using the Python `CVXPY`
library [[3](#Diamond2016)], we can setup the above objective and constraint as-is and directly obtain $\vec{w}$. Since
we have two features, the separating hyperplane is a line defined by

<span id="eq-objective" class="mx-auto">
$
\begin{equation}
  g(x) = -\frac{w_1}{w_2}x - \frac{b}{w_2}
\end{equation}
$
</span>

so we can plot the margins on opposite sides of the decision line using trigonometric identities:

<span id="eq-objective" class="mx-auto">
$
\begin{equation}
  g_\pm(x) = g(x) \pm m \sqrt{1 + \left(\frac{w_1}{w_2}\right)^2}
  \end{equation}
$
</span>

Figure&nbsp;[1](#fig-svm-db) shows the optimum decision boundary with maximized margins in the $a^*-b^*$ feature space,
along with the input data points and support vectors.

<figure id="fig-svm-db" class="container">
  <img
    src={buildCldUrl("svip/186/16-SVM/svm-db.png")}
    alt="SVM decision boundary"
    class="mx-auto"
  />
  <figcaption class="text-center">
    Figure 1: Decision boundary for oranges and apples data in $a^*-b^*$ feature space. The yellow region corresponds to
    a classification of `-1` (oranges), while the red region corresponds to `+1` (apples). The vectors highlighted in
    blue are the support vectors. The solid line is the decision boundary, while the dashed lines are the margins.
  </figcaption>
</figure>

#### References

<ol className="reference">
  <li id="Soriano2019">M. N. Soriano, _A16 - Support vector machines_ (2019).</li>
  <li id="Veksler">O. Veksler, _CS434a/541a: Pattern recognition - lecture 11_ (n.d.).</li>
  <li id="Diamond2016">
    S. Diamond, and S. Boyd, CVXPY: A Python-embedded modeling language for convex optimization,
    _Journal of Machine Learning Research_ **17**(83), 1-5 (2016).
  </li>
</ol>
