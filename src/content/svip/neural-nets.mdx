---
title: "Activity 17: Neural Networks"
keywords:
  - image processing
  - computer vision
  - neural networks
cover: "svip/186/17-NeuralNetworks/multiple-decision.png"
courseSlug: "ap186"
created: "2019-11-28T04:41:00.0000Z"

alphabet: abcdefghijklmnopqrstuvwxyz

functionData: [
  { url: "fit-1-pred.png", caption: "linear", key: "fig-linear" },
  { url: "fit-2-pred.png", caption: "quadratic", key: "fig-quadratic" },
  { url: "fit-3-pred.png", caption: "cubic", key: "fig-cubic" },
  { url: "fit-tanh-pred.png", caption: "tanh", key: "fig-tanh" },
  { url: "fit-sigmoid-pred.png", caption: "logistic sigmoid", key: "fig-sigmoid" },
  { url: "fit-sin1Hz-pred.png", caption: "sine", key: "fig-sine" },
]
---

import { buildCldUrl } from "@/utils/cloudinary";

#### Function fitter

For this activity [[1](#Soriano2019)], we will first explore the applications of a neural network---or more accurately,
a multilayer perceptron (MLP)---as a universal function fitter. Designing a neural network architecture can be quite
tedious as there are a lot of hyperparameters to tune in order for you to get desired results. After several attempts of
trial-and-error, I ended up with the network shown in Fig.&nbsp;[1](#fig-funcfit-architecture) (all network
visualizations were done using [[2](#LeNail2019)]). For the training data, I generated 1000 random numbers distributed
uniformly in the range $[-1, 1]$ as the input, and the corresponding function as the output. Specifically, the functions
used were:

- linear: $f(x) = x$
- quadratic: $f(x) = x^2$
- cubic: $f(x) = x^3$
- $\tanh(x)$
- logistic sigmoid: $(1 + e^x)^{-1}$
- $\sin(2\pi fx)$

<figure id="fig-funcfit-architecture" class="container">
  <img
    src={buildCldUrl("svip/186/17-NeuralNetworks/func-fit.png")}
    alt="Network architecture for the function fitter."
  />
  <figcaption class="col-span-3 text-center">
    Figure 1: Network architecture for the function fitter.
  </figcaption>
</figure>

The first hidden layer of the network contains 500 nodes, which are all activated by a rectified linear unit (ReLU),
defined by

<span id="eq-relu" class="mx-auto">
$
\begin{align}
  g(x) &= \max(0, x) \label{eq:relu} \\
	g^\prime(x) &=
	\begin{cases}
    0 & , \quad x < 0 \\
      1 & , \quad x > 0
	\end{cases} \label{eq:relu-grad}
\end{align}
$
</span>

The second hidden layer contains 10 nodes, also activated by ReLU. The output layer is a single node which maps
everything calculated so far to a single output value. Thus, the function arguments (random numbers $\in [-1,1]$)
comprise an entire dataset $\mathcal{M}$. The network objective is to minimize the $\ell_2$ loss, defined as

<span id="eq-l2-loss" class="mx-auto">
$
\begin{equation}
  \ell_2(\vec{y}, \hat{\vec{y}}) = \frac{1}{2} \left|\hat{\vec{y}} - \vec{y}\right|_2^2
\end{equation}
$
</span>

where $\vec{y}, \hat{\vec{y}}$ are the ground truth and predicted outputs, respectively, and
$\left|\vec{y}\right|_2 := \left(\sum_i y_i^2\right)^{1/2}$ is the $\ell_2$ norm operator. The network is trained until
the loss drops below a value of 0.01. I opted to use the $\ell_2$ loss instead of the sum-of-squares error or
mean-squared error because, from experience, the former is more perceptually accurate.

For the test data, I generated 1000 equally-spaced numbers in the range $[-1,1]$. Figure&nbsp;[2](#fig-function-fitter)
shows the predictions for each function.

<figure id="fig-function-fitter" class="container grid grid-cols-3 gap-4 items-end">
  {frontmatter.functionData.map((d, index) => (
    <figure id={d.key}>
      <img
        src={buildCldUrl(`svip/186/17-NeuralNetworks/${d.url}`)}
        alt={d.caption}
      />
      <figcaption class="text-center">
        ({frontmatter.alphabet[index]}) {d.caption}
      </figcaption>
    </figure>
  ))}
  <figcaption class="col-span-3 text-center">
    Figure 2: Neural network as a universal function fitter.
  </figcaption>
</figure>

#### Fruit classification

For this section, I used my extracted $a^*-b^*$ color features of bananas, apples, and oranges from previous activities.
The architecture I came up with is shown in Fig.&nbsp;[3](#fig-fruit-architecture). The dataset consists of 50 feature
vectors for each fruit (total of 150 feature vectors), which were split 80% for training and 20% for testing.

<figure id="fig-fruit-architecture" class="container">
  <img
    src={buildCldUrl("svip/186/17-NeuralNetworks/fruit-net.png")}
    alt="Network architecture for the fruit classifier"
  />
  <figcaption class="col-span-3 text-center">
    Figure 3: Network architecture for the fruit classifier.
  </figcaption>
</figure>

The network was designed such that the first hidden layer contains as many nodes as data points. Layers are added which
contain $\frac{1}{10}$ many nodes as the previous layer, until the number of nodes in the current layer is on the order
of magnitude of 10<sup>1</sup>. The output layer contains as many nodes as the classes. The hidden layers are activated
by a ReLU except for the last, which is activated by a logistic sigmoid, defined as

<span id="eq-sigmoid" class="mx-auto">
$
\begin{align}
  g(x) &= \frac{1}{1 + e^{-x}} \\
	g^\prime(x) &= g(x)(1 - g(x))
\end{align}
$
</span>

and the output layer is activated by a softmax, defined by

<span id="eq-softmax" class="mx-auto">
$
\begin{align}
  g(x_i) &= \frac{e^{x_i}}{\sum_j e^{x_j}} \label{eq:softmax} \\
	g^\prime(x_i) &= g(x_i)(\delta_{ik} - g(x_k))
\end{align}
$
</span>

which outputs probabilities such that the sum of the output nodes should equal unity. Thus, for this dataset, the
network topology is `120-12-3` (two hidden layers, 3 nodes in output layer).

This time, the network objective is to minimize the mean-squared error (MSE), and the network is trained until it drops
below a value of 0.01. Feeding in the test data shows a test accuracy of 100%, and the decision boundaries are shown
in Fig.&nbsp;[4](#fig-fruit-decision3). This is not surprising since the fruit feature vectors form distinct clusters
and do not overlap.

<figure id="fig-fruit-decision3" class="container">
  <img
    src={buildCldUrl("svip/186/17-NeuralNetworks/ban-app-ora-decision.png")}
    alt="Decision boundaries for bananas, apples, and oranges in a*-b* feature space"
    class="mx-auto"
  />
  <figcaption class="text-center">
    Figure 4: Decision boundaries for bananas, apples, and oranges in $a^*-b^*$ feature space.
  </figcaption>
</figure>

Now, let's consider adding more samples to the training/testing sets. Using the fruits dataset from [[3](#Muresan2018)],
I added more varieties of apples, bananas, and oranges, as well as an additional class of cherries.
Figure&nbsp;[5](#fig-fruits-set) shows the distribution of their feature vectors in $a^*-b^*$ space. Notice now that the
clusters occupy more area and are almost touching each other.

<figure id="fig-fruits-set" class="container">
  <img
    src={buildCldUrl("svip/186/17-NeuralNetworks/fruits-train.png")}
    alt="a*-b* feature space of apples, bananas, oranges, and cherries"
    class="mx-auto"
  />
  <figcaption class="text-center">
    Figure 5: $a^*-b^*$ feature space of apples, bananas, oranges, and cherries.
  </figcaption>
</figure>

With a total of 4752 feature vectors, I split the data 50-50 among the train-test sets. Therefore, the network topology
now is `2376-237-23-4` (3 hidden layers, 4 output nodes). Plugging in the test data afterwards yields a test accuracy of
99.96%, and the decision boundaries are shown in Fig.&nbsp;[6](#fig-multiple-fruits).

<figure id="fig-multiple-fruits" class="container">
  <img
    src={buildCldUrl("svip/186/17-NeuralNetworks/multiple-decision.png")}
    alt="Decision boundaries for bananas, apples, oranges, and cherries in a*-b* feature space"
    class="mx-auto"
  />
  <figcaption class="text-center">
    Figure 6: Decision boundaries for bananas, apples, oranges, and cherries in $a^*-b^*$ feature space.
  </figcaption>
</figure>

#### References

<ol className="reference">
  <li id="Soriano2019">M. N. Soriano, _A17 - Neural networks_ (2019).</li>
  <li id="LeNail2019">
    A. LeNail, Publication-ready neural network architecture schematics,
    _Journal of Open Source Software_ **4**(33) 747 (2019).
  </li>
  <li id="Muresan2018">
    H. Muresan, and M. Oltean, Fruit recognition from images using deep learning.
    _Acta Univ. Sapientiae, Informatica_ **10**(1), 26-42 (2018).
  </li>
</ol>
